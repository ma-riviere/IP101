{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:Navy\"> \n",
    "\n",
    "<div style=\"text-align:center\"> \n",
    "\n",
    "***\n",
    "# <u>TP5:</u>\n",
    "# Segmentation d'images\n",
    "    \n",
    "<p style=\"text-align: center; color:gray\"><i>@Author:</i> Marc-Aur√®le Rivi√®re</p>\n",
    "\n",
    "***\n",
    "        \n",
    "</div>\n",
    "    \n",
    "<u>**Plan:**</u>\n",
    "\n",
    "1. [**Segmentation par seuillage simple**](#1): \n",
    "    * En luminance: thresholding, m√©thode d'Otsu, m√©thodes adaptatives\n",
    "    * En chrominance\n",
    "\n",
    "    \n",
    "2. [**Segmentation non-supervis√©e**](#2): Mean-Shift, Watershed, Superpixels\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Colaboratory\n"
     ]
    }
   ],
   "source": [
    "'''''''''''''''''''''''''''''''''\n",
    "#################################\n",
    "#  Code global pour tout le TP  #\n",
    "#################################\n",
    "'''''''''''''''''''''''''''''''''\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running on Colaboratory\")\n",
    "    from google.colab import drive, files\n",
    "\n",
    "    drive.mount('/content/gdrive', force_remount=True)\n",
    "    root_path = 'gdrive/My Drive/3. Doctorat/Enseignements/[Intro] Image Processing/TP5/'  # A modifier √† votre chemin d'acc√®s\n",
    "    img_path = root_path + \"img/\"\n",
    "else:\n",
    "    print(\"Not running on Colaboratory\")\n",
    "    root_path = \"/\"\n",
    "    img_path = \"img/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: Green;text-decoration: underline\" id=\"1\">I. Segmentation par seuillage simple</span>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seuiller** une image (*thresholding*) consiste √† discr√©tiser l'√©chelle des valeurs de luminance de ses pixels selon un (ou plusieurs) seuils donn√©s.\n",
    "- Tout pixel avec une valeure sup√©rieure au seuil deviendra 255 (blanc)\n",
    "- Tout pixel avec une valeure inf√©rieure au seuil deviendra 0 (noir)\n",
    "\n",
    "Il existe **plusieurs types d'algorithmes de segmentation**:\n",
    "* __(Supervis√©, bottom-up):__ n√©cessitent de fournir les seuils manuellement: _thresholding_\n",
    "* __(Non-supervis√©, bottom-up):__ d√©terminent les seuils √† partir du contenu de l'image (luminance, chrominance)\n",
    "  * Thresholding non-supervis√©: _adaptative thresholding, Otsu's method_\n",
    "  * M√©thodes de clustering: _mean-shift (QuickShift), watershed, super-pixels (SLIC)_\n",
    "*  __(Supervis√©, top-down):__ d√©terminent les fronti√®res de segmentation √† partir de classes ayant un sens s√©mantique (g√©n√©ralement d√©finies par l'Homme), apprises lors d'une phase d'entrainement\n",
    "  * Machine Learning: _K-NN, SVM,..._\n",
    "  * Deep Learning: _U-Net, Mask-R-CNN, Deeplab,..._\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2001/1*zKnOz-YWIKtIohhYcydNEQ.png\">\n",
    "\n",
    "\n",
    "Pour cette premi√®re partie, nous allons nous focaliser sur les solutions de **segmentation par seuillage (supervis√©es ou non)**, en luminance et en chrominance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports et fonctions utiles √† cette partie\n",
    "\n",
    "import os, cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from skimage.morphology import erosion, closing, square, remove_small_holes\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.filters import threshold_multiotsu\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.color import label2rgb\n",
    "\n",
    "from matplotlib.pylab import *\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hsv_to_rgb, rgb_to_hsv\n",
    "%matplotlib inline\n",
    "# Am√©liorer la nettet√© des figures\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = 12.8, 9.6\n",
    "\n",
    "# \"Do not disturb\" mode\n",
    "import warnings                                        \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Interactivit√©\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, interactive_output\n",
    "from IPython.display import display, Markdown\n",
    "import emoji\n",
    "\n",
    "'''\n",
    "M√©thodes d'affichage\n",
    "'''\n",
    "def affichage_auto_simple(results, titles, cm=\"viridis\"):\n",
    "    size = len(results)\n",
    "    plt.figure(figsize=(6*size, 6))\n",
    "    for j, _res in enumerate(results):\n",
    "        plt.subplot(f\"1{size}{j+1}\")\n",
    "        plt.imshow(_res, cmap=cm, origin=\"upper\")\n",
    "        plt.title(titles[j] if titles != None and j < len(titles) else f\"Image {j+1}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def affichage_auto(original, processed, titles, breakpoint=3, cm=\"gray\"):\n",
    "    row = breakpoint\n",
    "    col = int(np.ceil(len(processed)/float(row)))\n",
    "    \n",
    "    # Affichage de l'image d'origine et de son histogramme\n",
    "    fig1 = plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(121), plt.imshow(original, cmap=cm, origin=\"upper\"), plt.title(\"Original\")\n",
    "    plt.subplot(122), plt.hist(np.array(original).ravel(), 255, [0,255]), plt.title(\"Histogramme\")\n",
    "    \n",
    "    # Affichage des diff√©rentes versions segment√©es \n",
    "    fig2 = plt.figure(figsize=(5*row, 5*col))\n",
    "    for i, img in enumerate(processed):\n",
    "        plt.subplot(f\"{col}{row}{i+1}\")\n",
    "        plt.imshow(img, cmap=cm, origin=\"upper\")\n",
    "        plt.title(titles[i] if titles != None and i < len(titles) else f\"Image {i}\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def affichage_otsu(original, result, thresh):\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(131), plt.imshow(original, cmap=cm, origin=\"upper\"), plt.title(\"Original\")\n",
    "    ax2 = plt.subplot(132)\n",
    "    ax2.hist(np.array(original).ravel(), 255, [0,255])\n",
    "    ax2.set_title(\"Histogramme\")\n",
    "    ax2.axvline(thresh, color='r')\n",
    "    plt.subplot(133), plt.imshow(result, cmap=cm, origin=\"upper\"), plt.title(\"Original\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def affichage_masks_and_bounds(original, bounds, masks):\n",
    "    \n",
    "    size_b = len(bounds) * 2\n",
    "    size_m = len(masks) + 1\n",
    "    \n",
    "    fig1 = plt.figure(figsize=(5*size_m, 5))\n",
    "    plt.subplot(f\"1{size_m}1\"), plt.imshow(original, origin=\"upper\"), plt.title(\"Original\")\n",
    "    for j, _mask in enumerate(masks):\n",
    "        plt.subplot(f\"1{size_m}{j+2}\"), plt.imshow(_mask, cmap=\"gray\"), plt.title(f\"Mask {j+1}\")\n",
    "    \n",
    "    fig2 = plt.figure(figsize=(3*size_b, 3))\n",
    "    c = 1\n",
    "    for i, _bound in enumerate(bounds):\n",
    "        lower = np.full((10, 10, 3), _bound[0]).astype(\"uint8\") / 255.0\n",
    "        upper = np.full((10, 10, 3), _bound[1]).astype(\"uint8\") / 255.0\n",
    "        plt.subplot(f\"1{size_b}{c}\"), plt.imshow(hsv_to_rgb(lower)), plt.title(f\"Lower {i+1}\")\n",
    "        plt.subplot(f\"1{size_b}{c+1}\"), plt.imshow(hsv_to_rgb(upper)), plt.title(f\"Upper {i+1}\")\n",
    "        c+=2\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: DodgerBlue;text-decoration: underline\">I.1 Segmentation en luminance</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1.a Thresholding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarisation avec OpenCV:\n",
    "\n",
    "```Python\n",
    "_, imageBinaire = cv2.threshold(img_gris, seuil, nouvelle_valeur, cv2.THRESH_BINARY)\n",
    "```\n",
    "\n",
    "**Avec:**\n",
    "- ```image```: l'image source (en niveaux de gris)\n",
    "- ```seuil```: seuil de binarisation, g√©n√©ralement (255-1)/2 = 127\n",
    "- ```nouvelle_valeur```: nouvelle valeur que vont prendre les pixels (g√©n√©ralement 255)\n",
    "\n",
    "Tous les pixels dont la valeur est > √† 127 prendront la valeur 255, et la valeur de 0 pour les autres.\n",
    "\n",
    "*OpenCV provides different types of thresholding which is given by the fourth parameter of the function. Basic thresholding as described above is done by using the type cv.THRESH_BINARY. All simple thresholding types are:*\n",
    "\n",
    "* `cv.THRESH_BINARY`\n",
    "* `cv.THRESH_BINARY_INV`\n",
    "* `cv.THRESH_TRUNC`\n",
    "* `cv.THRESH_TOZERO`\n",
    "* `cv.THRESH_TOZERO_INV`\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1730/1*gmOL367EAnlsSdtNFqjB-A.png\">\n",
    "<img src=\"https://miro.medium.com/max/1095/1*swjBYQOnuNfv1rHM3p39PQ.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d20bb90a194236a2582b4fa8044c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=1, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Comparaison des diff√©rents types de segmentation simple d'OpenCV\n",
    "\n",
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='adapt2.jfif', description='Image:')\n",
    "seuil_slider = widgets.IntSlider(min=50, max=250, step=5, value=127, continuous_update=False)\n",
    "\n",
    "@interact\n",
    "def binarisation(image=images_dropdown, seuil=seuil_slider):\n",
    "    \n",
    "    img = np.array(Image.open(img_path + image).convert(\"L\")).astype(\"uint8\")\n",
    "    \n",
    "    _,thresh1 = cv2.threshold(img,seuil,255,cv2.THRESH_BINARY)\n",
    "    _,thresh2 = cv2.threshold(img,seuil,255,cv2.THRESH_BINARY_INV)\n",
    "    _,thresh3 = cv2.threshold(img,seuil,255,cv2.THRESH_TRUNC)\n",
    "    _,thresh4 = cv2.threshold(img,seuil,255,cv2.THRESH_TOZERO)\n",
    "    _,thresh5 = cv2.threshold(img,seuil,255,cv2.THRESH_TOZERO_INV)\n",
    "\n",
    "    titles = ['BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']\n",
    "    images = [thresh1, thresh2, thresh3, thresh4, thresh5]\n",
    "\n",
    "    affichage_auto(img, images, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1.b Otsu's thresholding :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La m√©thode d'Otsu est une m√©thode de **seuillage simple non-supervis√©** qui permet de d√©terminer la valeur du seuil global de binarisation en analysant l'histogramme de l'image N&B: l'algorithme choisira automatiquement le seuil de sorte √† rendre la r√©partition des pixels de chaque c√¥t√© de la fronti√®re la plus √©quitable possible.\n",
    "\n",
    "<img src=\"https://scikit-image.org/docs/dev/_images/sphx_glr_plot_thresholding_0011.png\">\n",
    "\n",
    "#### Avec OpenCV:\n",
    "\n",
    "Il suffit de rajouter le \"flag\" `cv2.THRESH_OTSU` √† l'une des m√©thodes de binarisation d'OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac0c264a2e14068af5cdea58eaf8901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=3, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seuil_slider = widgets.IntSlider(min=101, max=201, step=2, value=127, continuous_update=False)\n",
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='cells.png', description='Image:')\n",
    "\n",
    "@interact\n",
    "def otsu(image=images_dropdown, seuil=seuil_slider):\n",
    "    img = np.array(Image.open(img_path + image).convert(\"L\")).astype(\"uint8\")\n",
    "\n",
    "    # Global thresholding\n",
    "    ret1,th1 = cv2.threshold(img, seuil, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Otsu's thresholding\n",
    "    ret2,th2 = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Otsu's thresholding after Gaussian filtering\n",
    "    blur = cv2.GaussianBlur(img, (5,5), 0)\n",
    "    ret3,th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    images = [th1, th2, th3]\n",
    "    titles = [f'Global Thresholding: {ret1}', f\"Otsu's Thresholding: {ret2}\", f\"Otsu's Thresholding after Gaussian Filt.: {ret3}\"]\n",
    "    affichage_auto(img, images, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Otsu avec `Skimage`:\n",
    "\n",
    "Il est possible de d√©terminer plusieurs seuils via la m√©thode d'Otsu, afin d'avoir une segmentation qui conserve plus d'informations sur la r√©partition des niveaux de luminance de l'image d'origine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cd245733b243f187aa380f5617d9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=3, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='cells.png', description='Image:')\n",
    "\n",
    "@interact\n",
    "def otsu(image=images_dropdown):\n",
    "    img = np.array(Image.open(img_path + image).convert(\"L\")).astype(\"uint8\")\n",
    "\n",
    "    thresholds = threshold_multiotsu(img)\n",
    "\n",
    "    # Using the threshold values, we generate the three regions.\n",
    "    regions = np.digitize(img, bins=thresholds)\n",
    "    \n",
    "    affichage_auto_simple([img, regions], [\"Original\", \"Multiple Otsu\"], cm=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1.c Adaptative Thresholding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les sections pr√©c√©dentes, l'on c'est limit√© √† une valeur globale pour binariser toute l'image (*global thresholding*). Dans certain cas (e.g. quand l'illumination de l'image varie √©normement dans diff√©rentes sections de l'image), cela produira de mauvais r√©sultats. La binarisation adaptative (*adaptative thresholding*) permet de rem√©dier √† ce probl√®me en **d√©terminant le seuil de mani√®re locale** (en se basant sur un certain nombre de voisins d'un pixel donn√©). Il en r√©sultera que diff√©rentes r√©gions de l'image seront binaris√©s √† diff√©rents seuils, en **prenant en compte le contexte local de luminance**.\n",
    "\n",
    "```python\n",
    "cv2.AdaptiveThreshold(src, [dst], maxValue, adaptive_method, thresholdType, blockSize, c)\n",
    "```\n",
    "\n",
    "<u>Avec</u> :\n",
    "* `maxValue`: nouvelle valeur pour les pixels qui passent le seuil d√©termin√© par la m√©thode adaptative (g√©n√©ralement 255).\n",
    "* `adaptiveMethod`: m√©thode √† utiliser pour d√©terminer le seuil (`ADAPTIVE_THRESH_MEAN_C` ou `ADAPTIVE_THRESH_GAUSSIAN_C`).\n",
    "* `thresholdType`: type de binarisation √† effectuer une fois le seuil d√©termin√© (`THRESH_BINARY` ou `THRESH_BINARY_INV`).\n",
    "* `blockSize`: taille du voisinnage √©valu√© pour d√©terminer le seuil (matrice carr√©e impaire: 3x3, 5x5, ...)\n",
    "* `C`: (optionel) constante soustraite de chaque valeur de seuil d√©termin√© pour les diff√©rentes r√©gions.\n",
    "  * _`cv2.ADAPTIVE_THRESH_MEAN_C`: seuil = moyenne des valeurs de luminance des pixels voisins, moins `C`._\n",
    "  * _`cv2.ADAPTIVE_THRESH_GAUSSIAN_C`: seuil = moyenne pond√©r√©e (selon un kernel Gaussien) des pixels voisins, moins `C`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626ae50619524af5970f843d127ed622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=1, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c_slider = widgets.IntSlider(min=0, max=125, step=5, value=5, continuous_update=False)\n",
    "kernel_size_slider = widgets.IntSlider(min=3, max=29, step=2, value=11, continuous_update=False)\n",
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='adapt2.jfif', description='Image:')\n",
    "\n",
    "@interact\n",
    "def adaptative_thresholding(image=images_dropdown, kernel_size=kernel_size_slider, c=c_slider):\n",
    "    img = np.array(Image.open(img_path + image).convert(\"L\")).astype(\"uint8\")\n",
    "\n",
    "    ret,th1 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "    th2 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, kernel_size, c)\n",
    "    th3 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, kernel_size, c)\n",
    "\n",
    "    images = [th1, th2, th3]\n",
    "    titles = [f'Global Thresholding: {ret}', 'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']\n",
    "    affichage_auto(img, images, titles, breakpoint=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: DodgerBlue;text-decoration: underline\">I.2 Segmentation en chrominance</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **segmentation en chrominance** √©tends les principes pr√©c√©dents aux images couleurs en les segmentant selon 3 seuils (pour chaque canal de l'image). \n",
    "\n",
    "Cette segmentation est g√©n√©ralement faite dans un espace colorim√©trique permet de manipuler chrominance et luminance s√©par√©ment, afin de pouvoir sp√©cifier des seuils de couleurs qui fonctionneront pour un grand nombres de valeurs de luminance.\n",
    "\n",
    "La plupart des m√©thodes de segmentation colorim√©triques demandent deux seuils pour chaque channel: une borne inf√©rieur et sup√©rieur.\n",
    "\n",
    "<u>Remarque</u>: outil utile pour pr√©lever les couleurs d'une image: https://pinetools.com/image-color-picker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avec un intervalle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f32471e68864ed598841cc634b2bbff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=5, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='coins2.jpg', description='Image:')\n",
    "h_slider = widgets.IntRangeSlider(value=[0, 179], min=0, max=179, step=1, description='Hue:', continuous_update=False)\n",
    "s_slider = widgets.IntRangeSlider(value=[18, 255], min=0, max=255, step=1, description='Saturation:', continuous_update=False)\n",
    "v_slider = widgets.IntRangeSlider(value=[18, 255], min=0, max=255, step=1, description='Value:', continuous_update=False)\n",
    "\n",
    "\n",
    "@interact\n",
    "def chrom_segment(image=images_dropdown, seuil_h=h_slider, seuil_s=s_slider, seuil_v=v_slider):\n",
    "    img = np.array(Image.open(img_path + image)).astype(\"uint8\")\n",
    "    \n",
    "    try:\n",
    "        img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    except:\n",
    "        print(\"Selectionner une image couleur !\")\n",
    "        return\n",
    "    \n",
    "    lower_bound = np.array([seuil_h[0], seuil_s[0], seuil_v[0]])\n",
    "    upper_bound = np.array([seuil_h[1], seuil_s[1], seuil_v[1]])\n",
    "    mask = cv2.inRange(img_hsv, lower_bound, upper_bound)\n",
    "    \n",
    "    # Nettoyage du mask par op√©rations morphologiques (skimage)\n",
    "    mask = remove_small_holes(mask, area_threshold=500)\n",
    "    mask = cv2.morphologyEx(np.array(mask).astype(\"uint8\"), cv2.MORPH_CLOSE, np.ones((3,3), dtype=\"int\"))\n",
    "    mask = clear_border(mask)\n",
    "    mask = cv2.erode(np.array(mask).astype(\"uint8\"), np.ones((3,3), dtype=\"int\"))\n",
    "    mask = np.array(mask).astype(\"uint8\")\n",
    "    \n",
    "    affichage_masks_and_bounds(img, [(lower_bound, upper_bound)], [mask])\n",
    "    \n",
    "    segment_button = widgets.Button(description='Segmenter', icon=\"check\", style=widgets.ButtonStyle(button_color='lightblue'))\n",
    "    display(segment_button)\n",
    "    display(Markdown(emoji.emojize(\"üëÜ Cliquez sur le bouton 'Segmenter' une fois vos seuils s√©lectionn√©s pour appliquer le masque\")))\n",
    "    \n",
    "    contour_button = widgets.Button(description='Bounding Box', icon=\"check\", style=widgets.ButtonStyle(button_color='lightgreen'))\n",
    "    display(contour_button)\n",
    "    display(Markdown(emoji.emojize(\"üëÜ Cliquez sur le bouton 'Bounding Box' une fois vos seuils s√©lectionn√©s pour cr√©er les Bounding Boxes\")))\n",
    "    \n",
    "    @segment_button.on_click\n",
    "    def do_segment(b):\n",
    "        masked = cv2.bitwise_and(img, img, mask=mask)\n",
    "        affichage_auto_simple([masked], [\"Masque de chrominance\"])\n",
    "    \n",
    "    @contour_button.on_click\n",
    "    def do_contours(b):\n",
    "        labels, n_labels = label(mask, return_num=True)\n",
    "        labeled_img = label2rgb(labels, image=img)\n",
    "        \n",
    "        props = regionprops(labels)\n",
    "        contoured = np.array(img.copy())\n",
    "        \n",
    "        for prop in props:\n",
    "            if prop.bbox_area > 0:\n",
    "                cv2.rectangle(contoured, (prop.bbox[1], prop.bbox[0]), (prop.bbox[3], prop.bbox[2]), (145, 0, 0), 2)\n",
    "\n",
    "        affichage_auto_simple([labeled_img, contoured], [f\"Labels: {n_labels}\", f\"Bounding boxes: {n_labels}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avec deux intervalles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a94f4391764c60accb992be488bce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AppLayout(children=(Dropdown(description='Image:', index=9, layout=Layout(grid_area='header'), options=('adapt‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d640a45a2ac54d768c64449b9996ca93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Cr√©ation de l'UI pour l'interaction avec l'utilisateur\n",
    "'''\n",
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='nemo.jpg', description='Image:')\n",
    "\n",
    "h_slider1 = widgets.IntRangeSlider(value=[1, 20], min=0, max=179, step=1, description='Hue:', continuous_update=False)\n",
    "s_slider1 = widgets.IntRangeSlider(value=[190, 255], min=0, max=255, step=1, description='Saturation:', continuous_update=False)\n",
    "v_slider1 = widgets.IntRangeSlider(value=[110, 255], min=0, max=255, step=1, description='Value:', continuous_update=False)\n",
    "box1 = widgets.VBox(children=[h_slider1, s_slider1, v_slider1])\n",
    "\n",
    "h_slider2 = widgets.IntRangeSlider(value=[0, 179], min=0, max=179, step=1, description='Hue:', continuous_update=False)\n",
    "s_slider2 = widgets.IntRangeSlider(value=[0, 90], min=0, max=255, step=1, description='Saturation:', continuous_update=False)\n",
    "v_slider2 = widgets.IntRangeSlider(value=[170, 255], min=0, max=255, step=1, description='Value:', continuous_update=False)\n",
    "box2 = widgets.VBox(children=[h_slider2, s_slider2, v_slider2])\n",
    "\n",
    "app = widgets.AppLayout(header=images_dropdown,\n",
    "          left_sidebar=box1,\n",
    "          center=None,\n",
    "          right_sidebar=box2,\n",
    "          footer=None,\n",
    "          justify_items='center', align_items='center')\n",
    "\n",
    "\n",
    "def clean_mask(mask, holes_thresh=200):\n",
    "    mask = remove_small_holes(mask, area_threshold=holes_thresh)\n",
    "    #mask = closing(mask, square(3))\n",
    "    mask = cv2.morphologyEx(np.array(mask).astype(\"uint8\"), cv2.MORPH_CLOSE, np.ones((3,3), dtype=\"int\"))\n",
    "    mask = clear_border(mask)\n",
    "    #mask = erosion(mask, square(3))\n",
    "    mask = cv2.erode(np.array(mask).astype(\"uint8\"), np.ones((3,3), dtype=\"int\"))\n",
    "    return np.array(mask).astype(\"uint8\")\n",
    "\n",
    "'''\n",
    "Fonction principale\n",
    "'''\n",
    "def chrom_segment2(image, h1,s1,v1, h2,v2,s2):\n",
    "    img = np.array(Image.open(img_path + image)).astype(\"uint8\")\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    # Orange\n",
    "    lower1 = np.array([h1[0], s1[0], v1[0]])\n",
    "    upper1 = np.array([h1[1], s1[1], v1[1]])\n",
    "    # Blanc\n",
    "    lower2 = np.array([h2[0], s2[0], v2[0]])\n",
    "    upper2 = np.array([h2[1], s2[1], v2[1]])\n",
    "\n",
    "    mask1 = cv2.inRange(img_hsv, lower1, upper1)\n",
    "    mask2 = cv2.inRange(img_hsv, lower2, upper2)\n",
    "    mask1 = clean_mask(mask1, holes_thresh=500)\n",
    "    mask2 = clean_mask(mask2, holes_thresh=500)\n",
    "    \n",
    "    affichage_masks_and_bounds(img, [(lower1,upper1), (lower2,upper2)], [mask1, mask2])\n",
    "    \n",
    "    mask = mask1 + mask2\n",
    "    result = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    final = cv2.drawContours(img, contours, -1, (0,255,0), 3)\n",
    "    \n",
    "    images = [result, final]\n",
    "    titles = [\"Segment√©\",\"Avec fronti√®res\"]\n",
    "    affichage_auto_simple(images, titles)\n",
    "    \n",
    "'''\n",
    "On associe la fonction principale √† l'UI pour r√©agir aux modifications de cette derni√®re\n",
    "'''\n",
    "out = interactive_output(chrom_segment2, {\"image\":images_dropdown, \"h1\":h_slider1, \"s1\":s_slider1, \"v1\":v_slider1, \n",
    "                                          \"h2\":h_slider2, \"s2\":s_slider2, \"v2\":v_slider2})\n",
    "display(app, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:crimson\">**[<u>Exercice</u>]** A vous de jouer:</span>\n",
    "***\n",
    "<div style=\"color:DarkSlateBlue\">\n",
    "    \n",
    "<span style=\"color:green\">**Le but de cet exercice est de g√©n√©rer un masque de segmentation pour une image, et de compter le nombre d'√©l√©ments qu'elle contient gr√¢ce √† ce masque.**</span>\n",
    "    \n",
    "1. **Effectuer un seuillage binaire sur l'image `coins3.jpg`:**\n",
    "    * Vous pouvez √©purer l'image d'origine et/ou le masque avec les op√©rations morphologiques appropri√©es.\n",
    "    * Vous pouvez √©ventuellement r√©duire les diff√©rences de luminance au sein des pi√®ces par des transformations de niveau de gris (min-max, egalisation / AHE, log, ...)\n",
    " \n",
    "    \n",
    "2. **R√©p√©tez l'op√©ration en utilisant un seuillage par intervalle de niveaux de gris.**\n",
    "    \n",
    "    \n",
    "3. **En vous basant sur le second exemple de la section I.2, √©laborez deux masques de segmentation de couleur pour l'image `coins2.jpg`, permettant chacun d'extraire une couleur de pi√®ce..**\n",
    "    * Ajoutez une option interactive permettant d'appliquer au choix l'un ou les deux masques √† l'image.\n",
    "    \n",
    "\n",
    "4. **A partir de la combinaison des deux masques pr√©c√©dents, tra√ßez les contours des pi√®ces.**\n",
    "        \n",
    "\n",
    "5. **Calculez la superficie des pi√®ces segment√©es.**\n",
    "\n",
    "    > <u>Astuce</u>:\n",
    "    ```python\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "    ```\n",
    "   \n",
    "6. **Faites encadrer les pi√®ces par une enveloppe englobante ellipsoidale et les afficher sur l'image d'origine.**\n",
    "    \n",
    "    > <u>Astuce</u>:\n",
    "    ```python\n",
    "    ellipse = cv2.fitEllipse(cnt)\n",
    "    cv2.ellipse(img, ellipse, (0,255,0), 2)\n",
    "    ```\n",
    "\n",
    "    \n",
    "7. **Dans une nouvelle liste, r√©organiser les pi√®ces par taille croissante et afficher un chiffre sur la pi√®ce repr√©sentant sa place dans cette liste.**\n",
    "    \n",
    "    > <u>Astuce</u>: vous pouvez √©crire au centre de l'ellipse correspondante (`ellipse.center`).\n",
    "\n",
    "\n",
    "8. **Modifiez votre code de sorte √† ce qu'il fonctionne √©galement pour l'image des cellules, et appliquez-le √† celle-ci afin de les segmenter, compter et mesurer.**\n",
    "    \n",
    "    > <u>Remarque</u>: la superficie extraite ainsi pourrait servir de feature basique pour une m√©thode de Machine Learning permettant de classifier / clusteriser les cellules.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# > Emplacement exercice <\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: Green;text-decoration: underline\" id=\"2\">II. Segmentation non-supervis√©e</span>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette section, nous allons nous int√©resser aux **m√©thodes automatis√©es ou semi-automatis√©es** de segmentation d'images, qui reposent g√©n√©ralement sur des algorithmes de **clustering**.\n",
    "\n",
    "Ces m√©thodes sont **non (ou faiblement) supervis√©es** et se basent √©galement sur des indices bas-niveau (*bottom-up*), comme la luminance et la chrominance, afin de d√©terminer les fronti√®res de segmentation les plus pertinentes entre les pixels. Ce sont des m√©thodes locales qui vont g√©n√©rer des fronti√®res multiples en comparant la valeur des pixels pour les regrouper en r√©gions \"similaires\". L'objectif de ces algorithmes est d'optimiser progressivement les r√©gions g√©n√©r√©es afin qu'elles correspondent √† des r√©gions ou objets naturelles, i.e. qui ont une **signification s√©mantique** pour l'Homme.\n",
    "\n",
    "L'utilisateur n'a plus de valeurs de seuils √† fournir, mais g√©n√©ralement un certain nombre de m√©ta-param√®tres qui vont guider l'algorithme dans sa recherche des seuils optimaux: nombre de culters, points de d√©part de la recherche (*markers*), ...\n",
    "\n",
    "<img src=\"https://filebox.ece.vt.edu/~jbhuang/teaching/ece5554-4554/fa16/images/slic.jpg\">\n",
    "\n",
    "\n",
    "Quelques exemples d'algorithmes:\n",
    "* Mean-Shift / Quickshift\n",
    "* Superpixels (*SLIC*)\n",
    "* Watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Imports et fonctions utiles √† cette partie\n",
    "'''\n",
    "\n",
    "import os, cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from skimage.segmentation import slic, quickshift, watershed, mark_boundaries\n",
    "from skimage.color import label2rgb\n",
    "\n",
    "from matplotlib.pylab import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Am√©liorer la nettet√© des figures\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = 12.8, 9.6\n",
    "\n",
    "# \"Do not disturb\" mode\n",
    "import warnings                                        \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Interactivit√©\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, interactive_output\n",
    "\n",
    "'''\n",
    "M√©thodes d'affichage\n",
    "'''\n",
    "def affichage_auto_simple(results, titles, cm=\"viridis\"):\n",
    "    size = len(results)\n",
    "    plt.figure(figsize=(6*size, 6))\n",
    "    for j, _res in enumerate(results):\n",
    "        plt.subplot(f\"1{size}{j+1}\")\n",
    "        plt.imshow(_res, cmap=cm, origin=\"upper\")\n",
    "        plt.title(titles[j] if titles != None and j < len(titles) else f\"Image {j+1}\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def affichage_auto(original, processed, titles, breakpoint=3, cm=\"gray\"):\n",
    "    row = breakpoint\n",
    "    col = int(np.ceil(len(processed)/float(row)))\n",
    "    \n",
    "    # Affichage de l'image d'origine et de son histogramme\n",
    "    fig1 = plt.figure(figsize=(6, 6))\n",
    "    plt.subplot(111), plt.imshow(original, cmap=cm, origin=\"upper\"), plt.title(\"Original\")\n",
    "    \n",
    "    # Affichage des diff√©rentes versions segment√©es \n",
    "    fig2 = plt.figure(figsize=(5*row, 5*col))\n",
    "    for i, img in enumerate(processed):\n",
    "        plt.subplot(f\"{col}{row}{i+1}\")\n",
    "        plt.imshow(img, cmap=cm, origin=\"upper\")\n",
    "        plt.title(titles[i] if titles != None and i < len(titles) else f\"Image {i}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: DodgerBlue;text-decoration: underline\">II.1 Mean-Shift filtering</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Mean shift filtering_** est un algorithme de culstering fr√©quemment utilis√© en traitement d'image. C'est un algorithme de recherche du \"mode\" local (d'un groupe de pixel) afin d'homog√©n√©iser ce groupe selon la valeur du mode. Il va \"applatir\" les gradients (hautes-fr√©quences) de texture (spatial) et de couleur.\n",
    "\n",
    "*For each pixel of an image (having a spatial location and a particular color), the set of neighboring pixels (within a spatial radius and a defined color distance) is determined. For this set of neighbor pixels, the new spatial center (spatial mean) and the new color mean value are calculated. These calculated mean values will serve as the new center for the next iteration. The described procedure will be iterated until the spatial and the color (or grayscale) mean stops changing. At the end of the iteration, the final mean color will be assigned to the starting position of that iteration.*\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/E9ItQ.png\">\n",
    "\n",
    "*The Mean Shift algorithm takes usually 3 inputs:*\n",
    "\n",
    "* _**A distance / similarity measure** for comparing pixels: L1 (Manhattan) or L2 (Euclidian) distance usually._\n",
    "* _**A radius / kernel size**: all pixels within this radius (measured according the above distance) will be accounted for the calculation._\n",
    "* _**A max value difference**: from all pixels inside the radius, we will take only those whose values are within this difference for calculating the mean._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec `OpenCV`:\n",
    "\n",
    "```python\n",
    "shifted = cv2.PyrMeanShiftFiltering(src, sp, sr)\n",
    "```\n",
    "\n",
    "Avec:\n",
    "* `sp`: la taille du kernel spatial.\n",
    "* `sr`: la taille du kernel de couleur.\n",
    "\n",
    "[Documentation Mean-Shift](https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html#pyrmeanshiftfiltering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff263bf82d8c49ceb5a7ca12a4a86e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=5, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='coins2.jpg', description='Image:')\n",
    "color_kernel_size_slider = widgets.IntSlider(min=5, max=71, step=2, value=51, continuous_update=False)\n",
    "kernel_size_slider = widgets.IntSlider(min=5, max=35, step=2, value=21, continuous_update=False)\n",
    "\n",
    "@interact\n",
    "def mean_shift(image=images_dropdown, kernel_size=kernel_size_slider, color_kernel_size=color_kernel_size_slider):\n",
    "    img = np.array(Image.open(img_path + image)).astype(\"uint8\")\n",
    "\n",
    "    cv2_mean_shift = cv2.pyrMeanShiftFiltering(img, sp=kernel_size, sr=color_kernel_size)\n",
    "\n",
    "    affichage_auto_simple([img, cv2_mean_shift], [\"Original\", \"Mean Shift\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec `Skimage`: \n",
    "\n",
    "_Quickshift has 4 main parameters:_\n",
    "* _`kernel_size` controls the size of the kernel used in smoothing the sample density. Higher means fewer clusters._\n",
    "* _`max_dist` controls the cut-off point for data distances. Higher means fewer clusters._\n",
    "* _`ratio` controls the trade-off between color-space proximity and image-space proximity. Higher values give more weight to color-space._\n",
    "* _`sigma` controls the width of the Gaussian smoothing as preprocessing (zero means no smoothing)._\n",
    "\n",
    "[Documentation Quickshift](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.quickshift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2487330f19fe4d1eaf5374d638ec740e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=5, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='coins2.jpg', description='Image:')\n",
    "kernel_size_slider = widgets.IntSlider(min=3, max=25, step=2, value=11, continuous_update=False)\n",
    "distance_slider = widgets.IntSlider(min=5, max=71, step=2, value=41, continuous_update=False)\n",
    "ratio_slider = widgets.FloatSlider(min=0.1, max=1.0, step=0.1, value=0.5, continuous_update=False)\n",
    "smoothing_slider = widgets.FloatSlider(min=0.0, max=20.0, step=1.0, value=5.0, continuous_update=False)\n",
    "\n",
    "\n",
    "@interact\n",
    "def quickshift(image=images_dropdown, kernel_s=kernel_size_slider, max_d=distance_slider, ratio_val=ratio_slider, smoothing_val=smoothing_slider):\n",
    "    img = np.array(Image.open(img_path + image)).astype(\"uint8\")\n",
    "    \n",
    "    segments_quick = quickshift(img, kernel_size=kernel_s, max_dist=max_d, sigma=smoothing_val)\n",
    "    img_quick = mark_boundaries(img, segments_quick)\n",
    "    homogenized_quick = label2rgb(segments_quick, img, kind='avg')\n",
    "    \n",
    "    affichage_auto_simple([img, img_quick, homogenized_quick], [\"Original\", f\"Quickshift clusters: {np.unique(segments_quick).size}\", \"Homogenized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: DodgerBlue;text-decoration: underline\">II.2 Watershed</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The **watershed algorithm** is a classic algorithm used for segmentation and is especially useful when extracting touching or overlapping objects in images.*\n",
    "\n",
    "<img src=\"https://opencv-python-tutroals.readthedocs.io/en/latest/_images/water_result.jpg\">\n",
    "\n",
    "*Instead of taking a color image as input, watershed requires a grayscale gradient image, where bright pixels denote a boundary between regions. The algorithm views the image as a landscape, with bright pixels forming high peaks. Indeed, any grayscale image can be viewed as a topographic surface where high intensity denotes peaks and hills while low intensity denotes valleys.*\n",
    "\n",
    "*Then, you start filling every isolated valleys (local minima) with different colored water (labels). As the water rises, depending on the peaks (gradients) nearby, water from different valleys (with different colors) will start to merge. To avoid that, you build barriers in the locations where water merges. You continue the work of filling water and building barriers until all the peaks are under water. Then the barriers you created gives you the segmentation result.*\n",
    "\n",
    "<img src=\"https://imagej.net/_images/thumb/c/c5/Watershed-flooding-graph.png/375px-Watershed-flooding-graph.png\">\n",
    "\n",
    "*When utilizing the watershed algorithm we must start with user-defined markers. These markers can be either manually defined via point-and-click, or we can automatically or heuristically define them using methods such as thresholding and/or morphological operations (such as the distance transform).*\n",
    "\n",
    "*Based on these markers, the watershed algorithm treats pixels in our input image as local elevation (called a topography) ‚Äî the method ‚Äúfloods‚Äù valleys, starting from the markers and moving outwards, until the valleys of different markers meet each other. In order to obtain an accurate watershed segmentation, those markers must be correctly placed : the initialisation phase has a huge impact on the algorithm results.*\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQxRc8v4c1VqKqMpGaq5qFW0FPiIFm4i9uVdnY-rThDki_9gzFV\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec `Skimage`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Watershed has 2 main parameters:_\n",
    "* _`markers` controls the desired number of markers, or an array marking the basins with the values to be assigned as markers -where zero means 'not a marker'). If `None` (no markers given), the local minima of the image are automatically used as markers._\n",
    "* _`compactness` controls the shape and overall distance between a pixel and its cluster center. Higher values result in more regularly-shaped watershed basins (usually squared)._\n",
    "\n",
    "[Documentation Watershed](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.watershed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2d0e5e3adc425c9881d355de241e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=5, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage.filters import sobel\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='coins2.jpg', description='Image:')\n",
    "markers_slider = widgets.IntSlider(min=10, max=300, step=10, value=100, continuous_update=False)\n",
    "compactness_slider = widgets.FloatSlider(min=1, max=10, step=1, value=1, continuous_update=False)\n",
    "\n",
    "@interact\n",
    "def watershed(image=images_dropdown, n_markers=markers_slider, compact=compactness_slider):\n",
    "    img = np.array(Image.open(img_path + image)).astype(\"uint8\")\n",
    "    img_sobel = sobel(rgb2gray(img))\n",
    "    \n",
    "    segments_watershed = watershed(img_sobel, markers=n_markers, compactness=compact/1000.0)\n",
    "    img_watershed = mark_boundaries(img, segments_watershed)\n",
    "    homogenized_watershed = label2rgb(segments_watershed, img, kind='avg')\n",
    "    \n",
    "    affichage_auto_simple([img, img_watershed, homogenized_watershed], [\"Original\", f\"Watershed clusters: {np.unique(segments_watershed).size}\", \"Homogenized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: DodgerBlue;text-decoration: underline\">II.3 Superpixels</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Many existing algorithms in computer vision use the pixel-grid as the underlying representation. The pixel-grid, however, is not a natural representation of visual scenes. It is rather an \"artifact\" of a digital imaging process. It would be more natural, and presumably more efficient, to work with perceptually meaningful entities obtained from a low-level grouping process.*\n",
    "\n",
    "*Superpixels are one answer to this problem: a superpixel can be defined as a group of pixels that share common characteristics (like pixel intensity ) :*\n",
    "* _They carry more information than pixels._\n",
    "* _They have a perceptual meaning since pixels belonging to a given superpixel share similar visual properties._\n",
    "* _They provide a convenient and compact representation of images that can be very useful for computationally demanding problems._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avec `Skimage`: SLIC (Simple Linear Iterative Clustering)\n",
    "\n",
    "Une impl√©mentation classique du principe des superpixels est l'algorithme **SLIC** : *this algorithm uses a machine learning algorithm called K-Means under the hood. It takes in all the pixel values of the image and tries to separate them out into the given number of sub-regions. It generates superpixels by clustering pixels based on their color similarity and proximity in the image plane.*\n",
    "\n",
    "*SLIC has 3 main paramters:*\n",
    "* _`n_segments` controls the (approximately desired number of regions in the output image._\n",
    "* _`compactness` controls the balances between color proximity and space proximity. Higher values give more weight to space proximity, making superpixel shapes more square/cubic._\n",
    "* _`sigma` controls the width of the Gaussian smoothing as preprocessing (zero means no smoothing)._\n",
    "\n",
    "\n",
    "[Documentation SLIC](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e76acb1d9d46afa0a3ce1654b6fa58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=5, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='coins2.jpg', description='Image:')\n",
    "segments_slider = widgets.IntSlider(min=10, max=300, step=10, value=100, continuous_update=False)\n",
    "compactness_slider = widgets.IntSlider(min=1, max=30, step=1, value=10, continuous_update=False)\n",
    "sigma_slider = widgets.IntSlider(min=1, max=15, step=1, value=5, continuous_update=False)\n",
    "\n",
    "@interact\n",
    "def superpixels_slic(image=images_dropdown, segments=segments_slider, compact=compactness_slider, sigma_val=sigma_slider):\n",
    "    img = np.array(Image.open(img_path + image)).astype(\"uint8\")\n",
    "    \n",
    "    segments_slic = slic(img, n_segments=segments, compactness=compact, sigma=sigma_val)\n",
    "    img_slic = mark_boundaries(img, segments_slic)\n",
    "    homogenized_slic = label2rgb(img_slic, img, kind='avg')\n",
    "    \n",
    "    affichage_auto_simple([img, img_slic, homogenized_slic], [\"Original\", f\"SLIC clusters: {np.unique(segments_slic).size}\", \"Homogenized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:crimson\">**[<u>Exercice</u>]** A vous de jouer:</span>\n",
    "***\n",
    "<div style=\"color:DarkSlateBlue\">\n",
    "    \n",
    "1. **Impl√©mentez une m√©thode interactive permettant de s√©lectionner l'un des 3 algorithmes de clustering pour l'appliquer √† l'image `cells.jpg`.**\n",
    "    \n",
    "    \n",
    "2. **Optimisez les param√®tres de la m√©thode afin d'obtenir la meilleure segmentation possible, et servez vous des clusters g√©nr√©s pour homog√©n√©iser l'image.**\n",
    "    \n",
    "    \n",
    "3. **A partir de l'image homog√©n√©is√©e, reprenez le code de l'exercice I. de ce TP afin d'obtenir un meilleur masque de segmentation.**\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# > Emplacement exercice <\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:Navy\"> \n",
    "\n",
    "***\n",
    "# Fin du TP5\n",
    "***\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
