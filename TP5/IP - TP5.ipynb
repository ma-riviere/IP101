{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:Navy\"> \n",
    "\n",
    "<div style=\"text-align:center\"> \n",
    "\n",
    "***\n",
    "# <u>TP5:</u>\n",
    "# Segmentation d'images\n",
    "    \n",
    "<p style=\"text-align: center; color:gray\"><i>@Author:</i> Marc-Aurèle Rivière</p>\n",
    "\n",
    "***\n",
    "        \n",
    "</div>\n",
    "    \n",
    "<u>**Plan:**</u>\n",
    "\n",
    "1. [**Segmentation par seuillage simple**](#1): \n",
    "    * En luminance: thresholding, méthode d'Otsu, méthodes adaptatives\n",
    "    * En chrominance\n",
    "\n",
    "    \n",
    "2. [**Segmentation non-supervisée**](#2): Mean-Shift, Watershed, Superpixels\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Colaboratory\n"
     ]
    }
   ],
   "source": [
    "'''''''''''''''''''''''''''''''''\n",
    "#################################\n",
    "#  Code global pour tout le TP  #\n",
    "#################################\n",
    "'''''''''''''''''''''''''''''''''\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running on Colaboratory\")\n",
    "    from google.colab import drive, files\n",
    "\n",
    "    drive.mount('/content/gdrive', force_remount=True)\n",
    "    root_path = 'gdrive/My Drive/3. Doctorat/Enseignements/[Intro] Image Processing/TP5/'  # A modifier à votre chemin d'accès\n",
    "    img_path = root_path + \"img/\"\n",
    "else:\n",
    "    print(\"Not running on Colaboratory\")\n",
    "    root_path = \"/\"\n",
    "    img_path = \"img/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: Green;text-decoration: underline\" id=\"1\">I. Segmentation par seuillage simple</span>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seuiller** une image (*thresholding*) consiste à discrétiser l'échelle des valeurs de luminance de ses pixels selon un (ou plusieurs) seuils donnés.\n",
    "- Tout pixel avec une valeure supérieure au seuil deviendra 255 (blanc)\n",
    "- Tout pixel avec une valeure inférieure au seuil deviendra 0 (noir)\n",
    "\n",
    "Il existe **plusieurs types d'algorithmes de segmentation**:\n",
    "* __(Supervisé, bottom-up):__ nécessitent de fournir les seuils manuellement: _thresholding_\n",
    "* __(Non-supervisé, bottom-up):__ déterminent les seuils à partir du contenu de l'image (luminance, chrominance)\n",
    "  * Thresholding non-supervisé: _adaptative thresholding, Otsu's method_\n",
    "  * Méthodes de clustering: _mean-shift (QuickShift), watershed, super-pixels (SLIC)_\n",
    "*  __(Supervisé, top-down):__ déterminent les frontières de segmentation à partir de classes ayant un sens sémantique (généralement définies par l'Homme), apprises lors d'une phase d'entrainement\n",
    "  * Machine Learning: _K-NN, SVM,..._\n",
    "  * Deep Learning: _U-Net, Mask-R-CNN, Deeplab,..._\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2001/1*zKnOz-YWIKtIohhYcydNEQ.png\">\n",
    "\n",
    "\n",
    "Pour cette première partie, nous allons nous focaliser sur les solutions de **segmentation par seuillage (supervisées ou non)**, en luminance et en chrominance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports et fonctions utiles à cette partie\n",
    "\n",
    "import os, cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from skimage.morphology import erosion, closing, square, remove_small_holes\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.filters import threshold_multiotsu\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.color import label2rgb\n",
    "\n",
    "from matplotlib.pylab import *\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hsv_to_rgb, rgb_to_hsv\n",
    "%matplotlib inline\n",
    "# Améliorer la netteté des figures\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = 12.8, 9.6\n",
    "\n",
    "# \"Do not disturb\" mode\n",
    "import warnings                                        \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Interactivité\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, interactive_output\n",
    "from IPython.display import display, Markdown\n",
    "import emoji\n",
    "\n",
    "'''\n",
    "Méthodes d'affichage\n",
    "'''\n",
    "def affichage_auto_simple(results, titles, cm=\"viridis\"):\n",
    "    size = len(results)\n",
    "    plt.figure(figsize=(6*size, 6))\n",
    "    for j, _res in enumerate(results):\n",
    "        plt.subplot(f\"1{size}{j+1}\")\n",
    "        plt.imshow(_res, cmap=cm, origin=\"upper\")\n",
    "        plt.title(titles[j] if titles != None and j < len(titles) else f\"Image {j+1}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def affichage_auto(original, processed, titles, breakpoint=3, cm=\"gray\"):\n",
    "    row = breakpoint\n",
    "    col = int(np.ceil(len(processed)/float(row)))\n",
    "    \n",
    "    # Affichage de l'image d'origine et de son histogramme\n",
    "    fig1 = plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(121), plt.imshow(original, cmap=cm, origin=\"upper\"), plt.title(\"Original\")\n",
    "    plt.subplot(122), plt.hist(np.array(original).ravel(), 255, [0,255]), plt.title(\"Histogramme\")\n",
    "    \n",
    "    # Affichage des différentes versions segmentées \n",
    "    fig2 = plt.figure(figsize=(5*row, 5*col))\n",
    "    for i, img in enumerate(processed):\n",
    "        plt.subplot(f\"{col}{row}{i+1}\")\n",
    "        plt.imshow(img, cmap=cm, origin=\"upper\")\n",
    "        plt.title(titles[i] if titles != None and i < len(titles) else f\"Image {i}\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def affichage_otsu(original, result, thresh):\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(131), plt.imshow(original, cmap=cm, origin=\"upper\"), plt.title(\"Original\")\n",
    "    ax2 = plt.subplot(132)\n",
    "    ax2.hist(np.array(original).ravel(), 255, [0,255])\n",
    "    ax2.set_title(\"Histogramme\")\n",
    "    ax2.axvline(thresh, color='r')\n",
    "    plt.subplot(133), plt.imshow(result, cmap=cm, origin=\"upper\"), plt.title(\"Original\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def affichage_masks_and_bounds(original, bounds, masks):\n",
    "    \n",
    "    size_b = len(bounds) * 2\n",
    "    size_m = len(masks) + 1\n",
    "    \n",
    "    fig1 = plt.figure(figsize=(5*size_m, 5))\n",
    "    plt.subplot(f\"1{size_m}1\"), plt.imshow(original, origin=\"upper\"), plt.title(\"Original\")\n",
    "    for j, _mask in enumerate(masks):\n",
    "        plt.subplot(f\"1{size_m}{j+2}\"), plt.imshow(_mask, cmap=\"gray\"), plt.title(f\"Mask {j+1}\")\n",
    "    \n",
    "    fig2 = plt.figure(figsize=(3*size_b, 3))\n",
    "    c = 1\n",
    "    for i, _bound in enumerate(bounds):\n",
    "        lower = np.full((10, 10, 3), _bound[0]).astype(\"uint8\") / 255.0\n",
    "        upper = np.full((10, 10, 3), _bound[1]).astype(\"uint8\") / 255.0\n",
    "        plt.subplot(f\"1{size_b}{c}\"), plt.imshow(hsv_to_rgb(lower)), plt.title(f\"Lower {i+1}\")\n",
    "        plt.subplot(f\"1{size_b}{c+1}\"), plt.imshow(hsv_to_rgb(upper)), plt.title(f\"Upper {i+1}\")\n",
    "        c+=2\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: DodgerBlue;text-decoration: underline\">I.1 Segmentation en luminance</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1.a Thresholding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarisation avec OpenCV:\n",
    "\n",
    "```Python\n",
    "_, imageBinaire = cv2.threshold(img_gris, seuil, nouvelle_valeur, cv2.THRESH_BINARY)\n",
    "```\n",
    "\n",
    "**Avec:**\n",
    "- ```image```: l'image source (en niveaux de gris)\n",
    "- ```seuil```: seuil de binarisation, généralement (255-1)/2 = 127\n",
    "- ```nouvelle_valeur```: nouvelle valeur que vont prendre les pixels (généralement 255)\n",
    "\n",
    "Tous les pixels dont la valeur est > à 127 prendront la valeur 255, et la valeur de 0 pour les autres.\n",
    "\n",
    "*OpenCV provides different types of thresholding which is given by the fourth parameter of the function. Basic thresholding as described above is done by using the type cv.THRESH_BINARY. All simple thresholding types are:*\n",
    "\n",
    "* `cv.THRESH_BINARY`\n",
    "* `cv.THRESH_BINARY_INV`\n",
    "* `cv.THRESH_TRUNC`\n",
    "* `cv.THRESH_TOZERO`\n",
    "* `cv.THRESH_TOZERO_INV`\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1730/1*gmOL367EAnlsSdtNFqjB-A.png\">\n",
    "<img src=\"https://miro.medium.com/max/1095/1*swjBYQOnuNfv1rHM3p39PQ.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d20bb90a194236a2582b4fa8044c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=1, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Comparaison des différents types de segmentation simple d'OpenCV\n",
    "\n",
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='adapt2.jfif', description='Image:')\n",
    "seuil_slider = widgets.IntSlider(min=50, max=250, step=5, value=127, continuous_update=False)\n",
    "\n",
    "@interact\n",
    "def binarisation(image=images_dropdown, seuil=seuil_slider):\n",
    "    \n",
    "    img = np.array(Image.open(img_path + image).convert(\"L\")).astype(\"uint8\")\n",
    "    \n",
    "    _,thresh1 = cv2.threshold(img,seuil,255,cv2.THRESH_BINARY)\n",
    "    _,thresh2 = cv2.threshold(img,seuil,255,cv2.THRESH_BINARY_INV)\n",
    "    _,thresh3 = cv2.threshold(img,seuil,255,cv2.THRESH_TRUNC)\n",
    "    _,thresh4 = cv2.threshold(img,seuil,255,cv2.THRESH_TOZERO)\n",
    "    _,thresh5 = cv2.threshold(img,seuil,255,cv2.THRESH_TOZERO_INV)\n",
    "\n",
    "    titles = ['BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']\n",
    "    images = [thresh1, thresh2, thresh3, thresh4, thresh5]\n",
    "\n",
    "    affichage_auto(img, images, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1.b Otsu's thresholding :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode d'Otsu est une méthode de **seuillage simple non-supervisé** qui permet de déterminer la valeur du seuil global de binarisation en analysant l'histogramme de l'image N&B: l'algorithme choisira automatiquement le seuil de sorte à rendre la répartition des pixels de chaque côté de la frontière la plus équitable possible.\n",
    "\n",
    "<img src=\"https://scikit-image.org/docs/dev/_images/sphx_glr_plot_thresholding_0011.png\">\n",
    "\n",
    "#### Avec OpenCV:\n",
    "\n",
    "Il suffit de rajouter le \"flag\" `cv2.THRESH_OTSU` à l'une des méthodes de binarisation d'OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac0c264a2e14068af5cdea58eaf8901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=3, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seuil_slider = widgets.IntSlider(min=101, max=201, step=2, value=127, continuous_update=False)\n",
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='cells.png', description='Image:')\n",
    "\n",
    "@interact\n",
    "def otsu(image=images_dropdown, seuil=seuil_slider):\n",
    "    img = np.array(Image.open(img_path + image).convert(\"L\")).astype(\"uint8\")\n",
    "\n",
    "    # Global thresholding\n",
    "    ret1,th1 = cv2.threshold(img, seuil, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Otsu's thresholding\n",
    "    ret2,th2 = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Otsu's thresholding after Gaussian filtering\n",
    "    blur = cv2.GaussianBlur(img, (5,5), 0)\n",
    "    ret3,th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    images = [th1, th2, th3]\n",
    "    titles = [f'Global Thresholding: {ret1}', f\"Otsu's Thresholding: {ret2}\", f\"Otsu's Thresholding after Gaussian Filt.: {ret3}\"]\n",
    "    affichage_auto(img, images, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Otsu avec `Skimage`:\n",
    "\n",
    "Il est possible de déterminer plusieurs seuils via la méthode d'Otsu, afin d'avoir une segmentation qui conserve plus d'informations sur la répartition des niveaux de luminance de l'image d'origine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cd245733b243f187aa380f5617d9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=3, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='cells.png', description='Image:')\n",
    "\n",
    "@interact\n",
    "def otsu(image=images_dropdown):\n",
    "    img = np.array(Image.open(img_path + image).convert(\"L\")).astype(\"uint8\")\n",
    "\n",
    "    thresholds = threshold_multiotsu(img)\n",
    "\n",
    "    # Using the threshold values, we generate the three regions.\n",
    "    regions = np.digitize(img, bins=thresholds)\n",
    "    \n",
    "    affichage_auto_simple([img, regions], [\"Original\", \"Multiple Otsu\"], cm=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1.c Adaptative Thresholding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les sections précédentes, l'on c'est limité à une valeur globale pour binariser toute l'image (*global thresholding*). Dans certain cas (e.g. quand l'illumination de l'image varie énormement dans différentes sections de l'image), cela produira de mauvais résultats. La binarisation adaptative (*adaptative thresholding*) permet de remédier à ce problème en **déterminant le seuil de manière locale** (en se basant sur un certain nombre de voisins d'un pixel donné). Il en résultera que différentes régions de l'image seront binarisés à différents seuils, en **prenant en compte le contexte local de luminance**.\n",
    "\n",
    "```python\n",
    "cv2.AdaptiveThreshold(src, [dst], maxValue, adaptive_method, thresholdType, blockSize, c)\n",
    "```\n",
    "\n",
    "<u>Avec</u> :\n",
    "* `maxValue`: nouvelle valeur pour les pixels qui passent le seuil déterminé par la méthode adaptative (généralement 255).\n",
    "* `adaptiveMethod`: méthode à utiliser pour déterminer le seuil (`ADAPTIVE_THRESH_MEAN_C` ou `ADAPTIVE_THRESH_GAUSSIAN_C`).\n",
    "* `thresholdType`: type de binarisation à effectuer une fois le seuil déterminé (`THRESH_BINARY` ou `THRESH_BINARY_INV`).\n",
    "* `blockSize`: taille du voisinnage évalué pour déterminer le seuil (matrice carrée impaire: 3x3, 5x5, ...)\n",
    "* `C`: (optionel) constante soustraite de chaque valeur de seuil déterminé pour les différentes régions.\n",
    "  * _`cv2.ADAPTIVE_THRESH_MEAN_C`: seuil = moyenne des valeurs de luminance des pixels voisins, moins `C`._\n",
    "  * _`cv2.ADAPTIVE_THRESH_GAUSSIAN_C`: seuil = moyenne pondérée (selon un kernel Gaussien) des pixels voisins, moins `C`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626ae50619524af5970f843d127ed622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=1, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c_slider = widgets.IntSlider(min=0, max=125, step=5, value=5, continuous_update=False)\n",
    "kernel_size_slider = widgets.IntSlider(min=3, max=29, step=2, value=11, continuous_update=False)\n",
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='adapt2.jfif', description='Image:')\n",
    "\n",
    "@interact\n",
    "def adaptative_thresholding(image=images_dropdown, kernel_size=kernel_size_slider, c=c_slider):\n",
    "    img = np.array(Image.open(img_path + image).convert(\"L\")).astype(\"uint8\")\n",
    "\n",
    "    ret,th1 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "    th2 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, kernel_size, c)\n",
    "    th3 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, kernel_size, c)\n",
    "\n",
    "    images = [th1, th2, th3]\n",
    "    titles = [f'Global Thresholding: {ret}', 'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']\n",
    "    affichage_auto(img, images, titles, breakpoint=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: DodgerBlue;text-decoration: underline\">I.2 Segmentation en chrominance</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **segmentation en chrominance** étends les principes précédents aux images couleurs en les segmentant selon 3 seuils (pour chaque canal de l'image). \n",
    "\n",
    "Cette segmentation est généralement faite dans un espace colorimétrique permet de manipuler chrominance et luminance séparément, afin de pouvoir spécifier des seuils de couleurs qui fonctionneront pour un grand nombres de valeurs de luminance.\n",
    "\n",
    "La plupart des méthodes de segmentation colorimétriques demandent deux seuils pour chaque channel: une borne inférieur et supérieur.\n",
    "\n",
    "<u>Remarque</u>: outil utile pour prélever les couleurs d'une image: https://pinetools.com/image-color-picker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avec un intervalle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f32471e68864ed598841cc634b2bbff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=5, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='coins2.jpg', description='Image:')\n",
    "h_slider = widgets.IntRangeSlider(value=[0, 179], min=0, max=179, step=1, description='Hue:', continuous_update=False)\n",
    "s_slider = widgets.IntRangeSlider(value=[18, 255], min=0, max=255, step=1, description='Saturation:', continuous_update=False)\n",
    "v_slider = widgets.IntRangeSlider(value=[18, 255], min=0, max=255, step=1, description='Value:', continuous_update=False)\n",
    "\n",
    "\n",
    "@interact\n",
    "def chrom_segment(image=images_dropdown, seuil_h=h_slider, seuil_s=s_slider, seuil_v=v_slider):\n",
    "    img = np.array(Image.open(img_path + image)).astype(\"uint8\")\n",
    "    \n",
    "    try:\n",
    "        img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    except:\n",
    "        print(\"Selectionner une image couleur !\")\n",
    "        return\n",
    "    \n",
    "    lower_bound = np.array([seuil_h[0], seuil_s[0], seuil_v[0]])\n",
    "    upper_bound = np.array([seuil_h[1], seuil_s[1], seuil_v[1]])\n",
    "    mask = cv2.inRange(img_hsv, lower_bound, upper_bound)\n",
    "    \n",
    "    # Nettoyage du mask par opérations morphologiques (skimage)\n",
    "    mask = remove_small_holes(mask, area_threshold=500)\n",
    "    mask = cv2.morphologyEx(np.array(mask).astype(\"uint8\"), cv2.MORPH_CLOSE, np.ones((3,3), dtype=\"int\"))\n",
    "    mask = clear_border(mask)\n",
    "    mask = cv2.erode(np.array(mask).astype(\"uint8\"), np.ones((3,3), dtype=\"int\"))\n",
    "    mask = np.array(mask).astype(\"uint8\")\n",
    "    \n",
    "    affichage_masks_and_bounds(img, [(lower_bound, upper_bound)], [mask])\n",
    "    \n",
    "    segment_button = widgets.Button(description='Segmenter', icon=\"check\", style=widgets.ButtonStyle(button_color='lightblue'))\n",
    "    display(segment_button)\n",
    "    display(Markdown(emoji.emojize(\"👆 Cliquez sur le bouton 'Segmenter' une fois vos seuils sélectionnés pour appliquer le masque\")))\n",
    "    \n",
    "    contour_button = widgets.Button(description='Bounding Box', icon=\"check\", style=widgets.ButtonStyle(button_color='lightgreen'))\n",
    "    display(contour_button)\n",
    "    display(Markdown(emoji.emojize(\"👆 Cliquez sur le bouton 'Bounding Box' une fois vos seuils sélectionnés pour créer les Bounding Boxes\")))\n",
    "    \n",
    "    @segment_button.on_click\n",
    "    def do_segment(b):\n",
    "        masked = cv2.bitwise_and(img, img, mask=mask)\n",
    "        affichage_auto_simple([masked], [\"Masque de chrominance\"])\n",
    "    \n",
    "    @contour_button.on_click\n",
    "    def do_contours(b):\n",
    "        labels, n_labels = label(mask, return_num=True)\n",
    "        labeled_img = label2rgb(labels, image=img)\n",
    "        \n",
    "        props = regionprops(labels)\n",
    "        contoured = np.array(img.copy())\n",
    "        \n",
    "        for prop in props:\n",
    "            if prop.bbox_area > 0:\n",
    "                cv2.rectangle(contoured, (prop.bbox[1], prop.bbox[0]), (prop.bbox[3], prop.bbox[2]), (145, 0, 0), 2)\n",
    "\n",
    "        affichage_auto_simple([labeled_img, contoured], [f\"Labels: {n_labels}\", f\"Bounding boxes: {n_labels}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avec deux intervalles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a94f4391764c60accb992be488bce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AppLayout(children=(Dropdown(description='Image:', index=9, layout=Layout(grid_area='header'), options=('adapt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d640a45a2ac54d768c64449b9996ca93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Création de l'UI pour l'interaction avec l'utilisateur\n",
    "'''\n",
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='nemo.jpg', description='Image:')\n",
    "\n",
    "h_slider1 = widgets.IntRangeSlider(value=[1, 20], min=0, max=179, step=1, description='Hue:', continuous_update=False)\n",
    "s_slider1 = widgets.IntRangeSlider(value=[190, 255], min=0, max=255, step=1, description='Saturation:', continuous_update=False)\n",
    "v_slider1 = widgets.IntRangeSlider(value=[110, 255], min=0, max=255, step=1, description='Value:', continuous_update=False)\n",
    "box1 = widgets.VBox(children=[h_slider1, s_slider1, v_slider1])\n",
    "\n",
    "h_slider2 = widgets.IntRangeSlider(value=[0, 179], min=0, max=179, step=1, description='Hue:', continuous_update=False)\n",
    "s_slider2 = widgets.IntRangeSlider(value=[0, 90], min=0, max=255, step=1, description='Saturation:', continuous_update=False)\n",
    "v_slider2 = widgets.IntRangeSlider(value=[170, 255], min=0, max=255, step=1, description='Value:', continuous_update=False)\n",
    "box2 = widgets.VBox(children=[h_slider2, s_slider2, v_slider2])\n",
    "\n",
    "app = widgets.AppLayout(header=images_dropdown,\n",
    "          left_sidebar=box1,\n",
    "          center=None,\n",
    "          right_sidebar=box2,\n",
    "          footer=None,\n",
    "          justify_items='center', align_items='center')\n",
    "\n",
    "\n",
    "def clean_mask(mask, holes_thresh=200):\n",
    "    mask = remove_small_holes(mask, area_threshold=holes_thresh)\n",
    "    #mask = closing(mask, square(3))\n",
    "    mask = cv2.morphologyEx(np.array(mask).astype(\"uint8\"), cv2.MORPH_CLOSE, np.ones((3,3), dtype=\"int\"))\n",
    "    mask = clear_border(mask)\n",
    "    #mask = erosion(mask, square(3))\n",
    "    mask = cv2.erode(np.array(mask).astype(\"uint8\"), np.ones((3,3), dtype=\"int\"))\n",
    "    return np.array(mask).astype(\"uint8\")\n",
    "\n",
    "'''\n",
    "Fonction principale\n",
    "'''\n",
    "def chrom_segment2(image, h1,s1,v1, h2,v2,s2):\n",
    "    img = np.array(Image.open(img_path + image)).astype(\"uint8\")\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    # Orange\n",
    "    lower1 = np.array([h1[0], s1[0], v1[0]])\n",
    "    upper1 = np.array([h1[1], s1[1], v1[1]])\n",
    "    # Blanc\n",
    "    lower2 = np.array([h2[0], s2[0], v2[0]])\n",
    "    upper2 = np.array([h2[1], s2[1], v2[1]])\n",
    "\n",
    "    mask1 = cv2.inRange(img_hsv, lower1, upper1)\n",
    "    mask2 = cv2.inRange(img_hsv, lower2, upper2)\n",
    "    mask1 = clean_mask(mask1, holes_thresh=500)\n",
    "    mask2 = clean_mask(mask2, holes_thresh=500)\n",
    "    \n",
    "    affichage_masks_and_bounds(img, [(lower1,upper1), (lower2,upper2)], [mask1, mask2])\n",
    "    \n",
    "    mask = mask1 + mask2\n",
    "    result = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    final = cv2.drawContours(img, contours, -1, (0,255,0), 3)\n",
    "    \n",
    "    images = [result, final]\n",
    "    titles = [\"Segmenté\",\"Avec frontières\"]\n",
    "    affichage_auto_simple(images, titles)\n",
    "    \n",
    "'''\n",
    "On associe la fonction principale à l'UI pour réagir aux modifications de cette dernière\n",
    "'''\n",
    "out = interactive_output(chrom_segment2, {\"image\":images_dropdown, \"h1\":h_slider1, \"s1\":s_slider1, \"v1\":v_slider1, \n",
    "                                          \"h2\":h_slider2, \"s2\":s_slider2, \"v2\":v_slider2})\n",
    "display(app, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:crimson\">**[<u>Exercice</u>]** A vous de jouer:</span>\n",
    "***\n",
    "<div style=\"color:DarkSlateBlue\">\n",
    "    \n",
    "<span style=\"color:green\">**Le but de cet exercice est de générer un masque de segmentation pour une image, et de compter le nombre d'éléments qu'elle contient grâce à ce masque.**</span>\n",
    "    \n",
    "1. **Effectuer un seuillage binaire sur l'image `coins3.jpg`:**\n",
    "    * Vous pouvez épurer l'image d'origine et/ou le masque avec les opérations morphologiques appropriées.\n",
    "    * Vous pouvez éventuellement réduire les différences de luminance au sein des pièces par des transformations de niveau de gris (min-max, egalisation / AHE, log, ...)\n",
    " \n",
    "    \n",
    "2. **Répétez l'opération en utilisant un seuillage par intervalle de niveaux de gris.**\n",
    "    \n",
    "    \n",
    "3. **En vous basant sur le second exemple de la section I.2, élaborez deux masques de segmentation de couleur pour l'image `coins2.jpg`, permettant chacun d'extraire une couleur de pièce..**\n",
    "    * Ajoutez une option interactive permettant d'appliquer au choix l'un ou les deux masques à l'image.\n",
    "    \n",
    "\n",
    "4. **A partir de la combinaison des deux masques précédents, traçez les contours des pièces.**\n",
    "        \n",
    "\n",
    "5. **Calculez la superficie des pièces segmentées.**\n",
    "\n",
    "    > <u>Astuce</u>:\n",
    "    ```python\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "    ```\n",
    "   \n",
    "6. **Faites encadrer les pièces par une enveloppe englobante ellipsoidale et les afficher sur l'image d'origine.**\n",
    "    \n",
    "    > <u>Astuce</u>:\n",
    "    ```python\n",
    "    ellipse = cv2.fitEllipse(cnt)\n",
    "    cv2.ellipse(img, ellipse, (0,255,0), 2)\n",
    "    ```\n",
    "\n",
    "    \n",
    "7. **Dans une nouvelle liste, réorganiser les pièces par taille croissante et afficher un chiffre sur la pièce représentant sa place dans cette liste.**\n",
    "    \n",
    "    > <u>Astuce</u>: vous pouvez écrire au centre de l'ellipse correspondante (`ellipse.center`).\n",
    "\n",
    "\n",
    "8. **Modifiez votre code de sorte à ce qu'il fonctionne également pour l'image des cellules, et appliquez-le à celle-ci afin de les segmenter, compter et mesurer.**\n",
    "    \n",
    "    > <u>Remarque</u>: la superficie extraite ainsi pourrait servir de feature basique pour une méthode de Machine Learning permettant de classifier / clusteriser les cellules.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# > Emplacement exercice <\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: Green;text-decoration: underline\" id=\"2\">II. Segmentation non-supervisée</span>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette section, nous allons nous intéresser aux **méthodes automatisées ou semi-automatisées** de segmentation d'images, qui reposent généralement sur des algorithmes de **clustering**.\n",
    "\n",
    "Ces méthodes sont **non (ou faiblement) supervisées** et se basent également sur des indices bas-niveau (*bottom-up*), comme la luminance et la chrominance, afin de déterminer les frontières de segmentation les plus pertinentes entre les pixels. Ce sont des méthodes locales qui vont générer des frontières multiples en comparant la valeur des pixels pour les regrouper en régions \"similaires\". L'objectif de ces algorithmes est d'optimiser progressivement les régions générées afin qu'elles correspondent à des régions ou objets naturelles, i.e. qui ont une **signification sémantique** pour l'Homme.\n",
    "\n",
    "L'utilisateur n'a plus de valeurs de seuils à fournir, mais généralement un certain nombre de méta-paramètres qui vont guider l'algorithme dans sa recherche des seuils optimaux: nombre de culters, points de départ de la recherche (*markers*), ...\n",
    "\n",
    "<img src=\"https://filebox.ece.vt.edu/~jbhuang/teaching/ece5554-4554/fa16/images/slic.jpg\">\n",
    "\n",
    "\n",
    "Quelques exemples d'algorithmes:\n",
    "* Mean-Shift / Quickshift\n",
    "* Superpixels (*SLIC*)\n",
    "* Watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Imports et fonctions utiles à cette partie\n",
    "'''\n",
    "\n",
    "import os, cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from skimage.segmentation import slic, quickshift, watershed, mark_boundaries\n",
    "from skimage.color import label2rgb\n",
    "\n",
    "from matplotlib.pylab import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Améliorer la netteté des figures\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = 12.8, 9.6\n",
    "\n",
    "# \"Do not disturb\" mode\n",
    "import warnings                                        \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Interactivité\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, interactive_output\n",
    "\n",
    "'''\n",
    "Méthodes d'affichage\n",
    "'''\n",
    "def affichage_auto_simple(results, titles, cm=\"viridis\"):\n",
    "    size = len(results)\n",
    "    plt.figure(figsize=(6*size, 6))\n",
    "    for j, _res in enumerate(results):\n",
    "        plt.subplot(f\"1{size}{j+1}\")\n",
    "        plt.imshow(_res, cmap=cm, origin=\"upper\")\n",
    "        plt.title(titles[j] if titles != None and j < len(titles) else f\"Image {j+1}\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def affichage_auto(original, processed, titles, breakpoint=3, cm=\"gray\"):\n",
    "    row = breakpoint\n",
    "    col = int(np.ceil(len(processed)/float(row)))\n",
    "    \n",
    "    # Affichage de l'image d'origine et de son histogramme\n",
    "    fig1 = plt.figure(figsize=(6, 6))\n",
    "    plt.subplot(111), plt.imshow(original, cmap=cm, origin=\"upper\"), plt.title(\"Original\")\n",
    "    \n",
    "    # Affichage des différentes versions segmentées \n",
    "    fig2 = plt.figure(figsize=(5*row, 5*col))\n",
    "    for i, img in enumerate(processed):\n",
    "        plt.subplot(f\"{col}{row}{i+1}\")\n",
    "        plt.imshow(img, cmap=cm, origin=\"upper\")\n",
    "        plt.title(titles[i] if titles != None and i < len(titles) else f\"Image {i}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: DodgerBlue;text-decoration: underline\">II.1 Mean-Shift filtering</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Mean shift filtering_** est un algorithme de culstering fréquemment utilisé en traitement d'image. C'est un algorithme de recherche du \"mode\" local (d'un groupe de pixel) afin d'homogénéiser ce groupe selon la valeur du mode. Il va \"applatir\" les gradients (hautes-fréquences) de texture (spatial) et de couleur.\n",
    "\n",
    "*For each pixel of an image (having a spatial location and a particular color), the set of neighboring pixels (within a spatial radius and a defined color distance) is determined. For this set of neighbor pixels, the new spatial center (spatial mean) and the new color mean value are calculated. These calculated mean values will serve as the new center for the next iteration. The described procedure will be iterated until the spatial and the color (or grayscale) mean stops changing. At the end of the iteration, the final mean color will be assigned to the starting position of that iteration.*\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/E9ItQ.png\">\n",
    "\n",
    "*The Mean Shift algorithm takes usually 3 inputs:*\n",
    "\n",
    "* _**A distance / similarity measure** for comparing pixels: L1 (Manhattan) or L2 (Euclidian) distance usually._\n",
    "* _**A radius / kernel size**: all pixels within this radius (measured according the above distance) will be accounted for the calculation._\n",
    "* _**A max value difference**: from all pixels inside the radius, we will take only those whose values are within this difference for calculating the mean._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec `OpenCV`:\n",
    "\n",
    "```python\n",
    "shifted = cv2.PyrMeanShiftFiltering(src, sp, sr)\n",
    "```\n",
    "\n",
    "Avec:\n",
    "* `sp`: la taille du kernel spatial.\n",
    "* `sr`: la taille du kernel de couleur.\n",
    "\n",
    "[Documentation Mean-Shift](https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html#pyrmeanshiftfiltering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff263bf82d8c49ceb5a7ca12a4a86e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=5, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='coins2.jpg', description='Image:')\n",
    "color_kernel_size_slider = widgets.IntSlider(min=5, max=71, step=2, value=51, continuous_update=False)\n",
    "kernel_size_slider = widgets.IntSlider(min=5, max=35, step=2, value=21, continuous_update=False)\n",
    "\n",
    "@interact\n",
    "def mean_shift(image=images_dropdown, kernel_size=kernel_size_slider, color_kernel_size=color_kernel_size_slider):\n",
    "    img = np.array(Image.open(img_path + image)).astype(\"uint8\")\n",
    "\n",
    "    cv2_mean_shift = cv2.pyrMeanShiftFiltering(img, sp=kernel_size, sr=color_kernel_size)\n",
    "\n",
    "    affichage_auto_simple([img, cv2_mean_shift], [\"Original\", \"Mean Shift\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec `Skimage`: \n",
    "\n",
    "_Quickshift has 4 main parameters:_\n",
    "* _`kernel_size` controls the size of the kernel used in smoothing the sample density. Higher means fewer clusters._\n",
    "* _`max_dist` controls the cut-off point for data distances. Higher means fewer clusters._\n",
    "* _`ratio` controls the trade-off between color-space proximity and image-space proximity. Higher values give more weight to color-space._\n",
    "* _`sigma` controls the width of the Gaussian smoothing as preprocessing (zero means no smoothing)._\n",
    "\n",
    "[Documentation Quickshift](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.quickshift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2487330f19fe4d1eaf5374d638ec740e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=5, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='coins2.jpg', description='Image:')\n",
    "kernel_size_slider = widgets.IntSlider(min=3, max=25, step=2, value=11, continuous_update=False)\n",
    "distance_slider = widgets.IntSlider(min=5, max=71, step=2, value=41, continuous_update=False)\n",
    "ratio_slider = widgets.FloatSlider(min=0.1, max=1.0, step=0.1, value=0.5, continuous_update=False)\n",
    "smoothing_slider = widgets.FloatSlider(min=0.0, max=20.0, step=1.0, value=5.0, continuous_update=False)\n",
    "\n",
    "\n",
    "@interact\n",
    "def quickshift(image=images_dropdown, kernel_s=kernel_size_slider, max_d=distance_slider, ratio_val=ratio_slider, smoothing_val=smoothing_slider):\n",
    "    img = np.array(Image.open(img_path + image)).astype(\"uint8\")\n",
    "    \n",
    "    segments_quick = quickshift(img, kernel_size=kernel_s, max_dist=max_d, sigma=smoothing_val)\n",
    "    img_quick = mark_boundaries(img, segments_quick)\n",
    "    homogenized_quick = label2rgb(segments_quick, img, kind='avg')\n",
    "    \n",
    "    affichage_auto_simple([img, img_quick, homogenized_quick], [\"Original\", f\"Quickshift clusters: {np.unique(segments_quick).size}\", \"Homogenized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: DodgerBlue;text-decoration: underline\">II.2 Watershed</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The **watershed algorithm** is a classic algorithm used for segmentation and is especially useful when extracting touching or overlapping objects in images.*\n",
    "\n",
    "<img src=\"https://opencv-python-tutroals.readthedocs.io/en/latest/_images/water_result.jpg\">\n",
    "\n",
    "*Instead of taking a color image as input, watershed requires a grayscale gradient image, where bright pixels denote a boundary between regions. The algorithm views the image as a landscape, with bright pixels forming high peaks. Indeed, any grayscale image can be viewed as a topographic surface where high intensity denotes peaks and hills while low intensity denotes valleys.*\n",
    "\n",
    "*Then, you start filling every isolated valleys (local minima) with different colored water (labels). As the water rises, depending on the peaks (gradients) nearby, water from different valleys (with different colors) will start to merge. To avoid that, you build barriers in the locations where water merges. You continue the work of filling water and building barriers until all the peaks are under water. Then the barriers you created gives you the segmentation result.*\n",
    "\n",
    "<img src=\"https://imagej.net/_images/thumb/c/c5/Watershed-flooding-graph.png/375px-Watershed-flooding-graph.png\">\n",
    "\n",
    "*When utilizing the watershed algorithm we must start with user-defined markers. These markers can be either manually defined via point-and-click, or we can automatically or heuristically define them using methods such as thresholding and/or morphological operations (such as the distance transform).*\n",
    "\n",
    "*Based on these markers, the watershed algorithm treats pixels in our input image as local elevation (called a topography) — the method “floods” valleys, starting from the markers and moving outwards, until the valleys of different markers meet each other. In order to obtain an accurate watershed segmentation, those markers must be correctly placed : the initialisation phase has a huge impact on the algorithm results.*\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQxRc8v4c1VqKqMpGaq5qFW0FPiIFm4i9uVdnY-rThDki_9gzFV\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec `Skimage`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Watershed has 2 main parameters:_\n",
    "* _`markers` controls the desired number of markers, or an array marking the basins with the values to be assigned as markers -where zero means 'not a marker'). If `None` (no markers given), the local minima of the image are automatically used as markers._\n",
    "* _`compactness` controls the shape and overall distance between a pixel and its cluster center. Higher values result in more regularly-shaped watershed basins (usually squared)._\n",
    "\n",
    "[Documentation Watershed](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.watershed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2d0e5e3adc425c9881d355de241e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=5, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage.filters import sobel\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='coins2.jpg', description='Image:')\n",
    "markers_slider = widgets.IntSlider(min=10, max=300, step=10, value=100, continuous_update=False)\n",
    "compactness_slider = widgets.FloatSlider(min=1, max=10, step=1, value=1, continuous_update=False)\n",
    "\n",
    "@interact\n",
    "def watershed(image=images_dropdown, n_markers=markers_slider, compact=compactness_slider):\n",
    "    img = np.array(Image.open(img_path + image)).astype(\"uint8\")\n",
    "    img_sobel = sobel(rgb2gray(img))\n",
    "    \n",
    "    segments_watershed = watershed(img_sobel, markers=n_markers, compactness=compact/1000.0)\n",
    "    img_watershed = mark_boundaries(img, segments_watershed)\n",
    "    homogenized_watershed = label2rgb(segments_watershed, img, kind='avg')\n",
    "    \n",
    "    affichage_auto_simple([img, img_watershed, homogenized_watershed], [\"Original\", f\"Watershed clusters: {np.unique(segments_watershed).size}\", \"Homogenized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: DodgerBlue;text-decoration: underline\">II.3 Superpixels</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Many existing algorithms in computer vision use the pixel-grid as the underlying representation. The pixel-grid, however, is not a natural representation of visual scenes. It is rather an \"artifact\" of a digital imaging process. It would be more natural, and presumably more efficient, to work with perceptually meaningful entities obtained from a low-level grouping process.*\n",
    "\n",
    "*Superpixels are one answer to this problem: a superpixel can be defined as a group of pixels that share common characteristics (like pixel intensity ) :*\n",
    "* _They carry more information than pixels._\n",
    "* _They have a perceptual meaning since pixels belonging to a given superpixel share similar visual properties._\n",
    "* _They provide a convenient and compact representation of images that can be very useful for computationally demanding problems._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avec `Skimage`: SLIC (Simple Linear Iterative Clustering)\n",
    "\n",
    "Une implémentation classique du principe des superpixels est l'algorithme **SLIC** : *this algorithm uses a machine learning algorithm called K-Means under the hood. It takes in all the pixel values of the image and tries to separate them out into the given number of sub-regions. It generates superpixels by clustering pixels based on their color similarity and proximity in the image plane.*\n",
    "\n",
    "*SLIC has 3 main paramters:*\n",
    "* _`n_segments` controls the (approximately desired number of regions in the output image._\n",
    "* _`compactness` controls the balances between color proximity and space proximity. Higher values give more weight to space proximity, making superpixel shapes more square/cubic._\n",
    "* _`sigma` controls the width of the Gaussian smoothing as preprocessing (zero means no smoothing)._\n",
    "\n",
    "\n",
    "[Documentation SLIC](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e76acb1d9d46afa0a3ce1654b6fa58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Image:', index=5, options=('adapt1.jpg', 'adapt2.jfif', 'barbara.j…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_dropdown = widgets.Dropdown(options=[f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))],\n",
    "    value='coins2.jpg', description='Image:')\n",
    "segments_slider = widgets.IntSlider(min=10, max=300, step=10, value=100, continuous_update=False)\n",
    "compactness_slider = widgets.IntSlider(min=1, max=30, step=1, value=10, continuous_update=False)\n",
    "sigma_slider = widgets.IntSlider(min=1, max=15, step=1, value=5, continuous_update=False)\n",
    "\n",
    "@interact\n",
    "def superpixels_slic(image=images_dropdown, segments=segments_slider, compact=compactness_slider, sigma_val=sigma_slider):\n",
    "    img = np.array(Image.open(img_path + image)).astype(\"uint8\")\n",
    "    \n",
    "    segments_slic = slic(img, n_segments=segments, compactness=compact, sigma=sigma_val)\n",
    "    img_slic = mark_boundaries(img, segments_slic)\n",
    "    homogenized_slic = label2rgb(img_slic, img, kind='avg')\n",
    "    \n",
    "    affichage_auto_simple([img, img_slic, homogenized_slic], [\"Original\", f\"SLIC clusters: {np.unique(segments_slic).size}\", \"Homogenized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:crimson\">**[<u>Exercice</u>]** A vous de jouer:</span>\n",
    "***\n",
    "<div style=\"color:DarkSlateBlue\">\n",
    "    \n",
    "1. **Implémentez une méthode interactive permettant de sélectionner l'un des 3 algorithmes de clustering pour l'appliquer à l'image `cells.jpg`.**\n",
    "    \n",
    "    \n",
    "2. **Optimisez les paramètres de la méthode afin d'obtenir la meilleure segmentation possible, et servez vous des clusters génrés pour homogénéiser l'image.**\n",
    "    \n",
    "    \n",
    "3. **A partir de l'image homogénéisée, reprenez le code de l'exercice I. de ce TP afin d'obtenir un meilleur masque de segmentation.**\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# > Emplacement exercice <\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:Navy\"> \n",
    "\n",
    "***\n",
    "# Fin du TP5\n",
    "***\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
